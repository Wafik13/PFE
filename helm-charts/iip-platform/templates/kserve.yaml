{{- if .Values.kserve.enabled }}
# KServe Controller Manager
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kserve-controller-manager
  namespace: iip-mlops
  labels:
    app.kubernetes.io/name: kserve-controller
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/version: {{ .Chart.AppVersion }}
    app.kubernetes.io/component: model-serving
    app.kubernetes.io/part-of: iip-platform
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: kserve-controller
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kserve-controller
        app.kubernetes.io/instance: {{ .Release.Name }}
        app.kubernetes.io/component: model-serving
    spec:
      serviceAccountName: iip-mlops-operator
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      containers:
      - name: manager
        image: "kserve/kserve-controller:v0.11.0"
        command:
        - /manager
        args:
        - --config=controller_manager_config.yaml
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: SECRET_NAME
          value: webhook-server-certs
        ports:
        - containerPort: 9443
          name: webhook-server
          protocol: TCP
        - containerPort: 8080
          name: metrics
          protocol: TCP
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 200Mi
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - mountPath: /tmp/k8s-webhook-server/serving-certs
          name: cert
          readOnly: true
        - mountPath: /controller_manager_config.yaml
          name: manager-config
          subPath: controller_manager_config.yaml
      volumes:
      - name: cert
        secret:
          defaultMode: 420
          secretName: webhook-server-certs
      - name: manager-config
        configMap:
          name: kserve-config

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kserve-config
  namespace: iip-mlops
  labels:
    app.kubernetes.io/name: kserve-controller
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/component: model-serving
data:
  controller_manager_config.yaml: |
    apiVersion: controller-runtime.sigs.k8s.io/v1alpha1
    kind: ControllerManagerConfig
    health:
      healthProbeBindAddress: :8081
    metrics:
      bindAddress: 127.0.0.1:8080
    webhook:
      port: 9443
    leaderElection:
      leaderElect: true
      resourceName: kserve-controller-manager
  inferenceservice: |
    {
        "storageInitializer": {
            "image": "kserve/storage-initializer:v0.11.0",
            "memoryRequest": "100Mi",
            "memoryLimit": "1Gi",
            "cpuRequest": "100m",
            "cpuLimit": "1"
        },
        "credentials": {
            "gcs": {
                "gcsCredentialFileName": "gcloud-application-credentials.json"
            },
            "s3": {
                "s3AccessKeyIDName": "AWS_ACCESS_KEY_ID",
                "s3SecretAccessKeyName": "AWS_SECRET_ACCESS_KEY",
                "s3Endpoint": "http://minio.iip-infrastructure.svc.cluster.local:9000",
                "s3UseHttps": "0",
                "s3Region": "us-east-1",
                "s3VerifySSL": "0"
            }
        },
        "predictors": {
            "tensorflow": {
                "image": "tensorflow/serving",
                "defaultImageVersion": "2.6.2",
                "defaultGpuImageVersion": "2.6.2-gpu",
                "defaultTimeout": "60",
                "supportedFrameworks": [
                    "tensorflow"
                ],
                "multiModelServer": false
            },
            "triton": {
                "image": "nvcr.io/nvidia/tritonserver",
                "defaultImageVersion": "23.04-py3",
                "defaultTimeout": "60",
                "supportedFrameworks": [
                    "tensorrt",
                    "tensorflow",
                    "onnx",
                    "pytorch"
                ],
                "multiModelServer": true
            },
            "pytorch": {
                "image": "pytorch/torchserve-kfs",
                "defaultImageVersion": "0.7.0",
                "defaultGpuImageVersion": "0.7.0-gpu",
                "defaultTimeout": "60",
                "supportedFrameworks": [
                    "pytorch"
                ],
                "multiModelServer": false
            },
            "sklearn": {
                "image": "kserve/sklearnserver",
                "defaultImageVersion": "v0.11.0",
                "defaultTimeout": "60",
                "supportedFrameworks": [
                    "sklearn"
                ],
                "multiModelServer": false
            }
        }
    }

{{- if .Values.kserve.models.predictiveMaintenance.enabled }}
---
# Predictive Maintenance Model InferenceService
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: predictive-maintenance-model
  namespace: iip-applications
  labels:
    app.kubernetes.io/name: predictive-maintenance-model
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: iip-platform
  annotations:
    serving.kserve.io/deploymentMode: Serverless
    autoscaling.knative.dev/target: "10"
    autoscaling.knative.dev/minScale: "1"
    autoscaling.knative.dev/maxScale: "10"
spec:
  predictor:
    serviceAccountName: iip-ml-inference
    triton:
      storageUri: {{ .Values.kserve.models.predictiveMaintenance.storageUri }}
      resources:
        {{- toYaml .Values.kserve.models.predictiveMaintenance.resources | nindent 8 }}
      env:
      - name: OMP_NUM_THREADS
        value: "1"
      - name: TRITON_MODEL_REPOSITORY
        value: "/mnt/models"
      nodeSelector:
        accelerator: nvidia-tesla-v100
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      affinity:
        nodeAffinity:
          {{- toYaml .Values.affinity.nodeAffinity.gpu | nindent 10 }}
{{- end }}

---
# ML Inference Service for Real-time Predictions
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference-service
  namespace: iip-applications
  labels:
    app.kubernetes.io/name: ml-inference-service
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/version: {{ .Chart.AppVersion }}
    app.kubernetes.io/component: ml-inference
    app.kubernetes.io/part-of: iip-platform
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: {{ .Values.microservices.mlInferenceService.replicaCount }}
  selector:
    matchLabels:
      app.kubernetes.io/name: ml-inference-service
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ml-inference-service
        app.kubernetes.io/instance: {{ .Release.Name }}
        app.kubernetes.io/component: ml-inference
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
        sidecar.istio.io/inject: "true"
    spec:
      serviceAccountName: iip-ml-inference
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - ml-inference-service
              topologyKey: kubernetes.io/hostname
        nodeAffinity:
          {{- toYaml .Values.affinity.nodeAffinity.gpu | nindent 10 }}
      tolerations:
        {{- toYaml .Values.tolerations.gpu | nindent 8 }}
      containers:
      - name: ml-inference
        image: "{{ .Values.microservices.mlInferenceService.image.repository }}:{{ .Values.microservices.mlInferenceService.image.tag }}"
        imagePullPolicy: Always
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        - name: grpc
          containerPort: 9090
          protocol: TCP
        env:
        - name: SERVICE_NAME
          value: "ml-inference-service"
        - name: MODEL_REPOSITORY
          value: "s3://iip-models/inference-models"
        - name: TRITON_SERVER_URL
          value: "predictive-maintenance-model.iip-applications.svc.cluster.local:8000"
        - name: FEAST_FEATURE_SERVER_URL
          value: "feast-feature-server.iip-mlops.svc.cluster.local:6566"
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow-server.iip-mlops.svc.cluster.local:5000"
        - name: REDIS_HOST
          value: "redis-master.iip-infrastructure.svc.cluster.local"
        - name: REDIS_PORT
          value: "6379"
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: redis
              key: redis-password
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: minio-credentials
              key: access-key
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio-credentials
              key: secret-key
        - name: AWS_S3_ENDPOINT
          value: "http://minio.iip-infrastructure.svc.cluster.local:9000"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
        resources:
          {{- toYaml .Values.microservices.mlInferenceService.resources | nindent 10 }}
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: model-cache
          mountPath: /app/model-cache
      volumes:
      - name: tmp
        emptyDir: {}
      - name: model-cache
        emptyDir:
          sizeLimit: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: ml-inference-service
  namespace: iip-applications
  labels:
    app.kubernetes.io/name: ml-inference-service
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/component: ml-inference
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: http
    protocol: TCP
    name: http
  - port: 9090
    targetPort: grpc
    protocol: TCP
    name: grpc
  selector:
    app.kubernetes.io/name: ml-inference-service
    app.kubernetes.io/instance: {{ .Release.Name }}

{{- if .Values.microservices.mlInferenceService.autoscaling.enabled }}
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-inference-service-hpa
  namespace: iip-applications
  labels:
    app.kubernetes.io/name: ml-inference-service
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/component: autoscaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-inference-service
  minReplicas: {{ .Values.microservices.mlInferenceService.autoscaling.minReplicas }}
  maxReplicas: {{ .Values.microservices.mlInferenceService.autoscaling.maxReplicas }}
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{ .Values.microservices.mlInferenceService.autoscaling.targetCPUUtilizationPercentage }}
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Resource
    resource:
      name: nvidia.com/gpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 1
        periodSeconds: 60
      selectPolicy: Max
{{- end }}
{{- end }}